---
layout: post
title: Would Asimov use ChatGPT ? A Threat Modeler's Take on AI Ethics
author: Izar Tarandach
---

<figure><img alt="" src="/assets/img/asimov.png" /></figure>

September 1st, 2025


# **Would Asimov use ChatGPT ? A Threat Modeler's Take on AI Ethics**

As a proto-geek, I grew up immersed in the science fiction of the “big three”: **Asimov, Clarke, Heinlein** (and yes, a good measure of visits to Mordor. And Ray Bradbury, can’t forget the man who put in my head the fantasy of a shuttle landing in the front yard and whisking me away to somewhere where you learned to fly spaceships. Way before I found Gibson and Sterling).  Back then, their stories felt like dispatches from some crazy future. Robots, faster-than-light travel, thinking machines, all comfortably sealed in the pages of paperback books. 

Time has this way of erasing the line between fiction and reality, if in memories or in predictions. What once read as fantasy now feels, uncomfortably so,  like the morning news. As a full-fledged geek and avid threat modeler, this blurry line evokes…thoughts.

Nowadays the one work that calls to me the most is not some dystopian reality, or even “Idiocracy”. It is **the** classic: Isaac Asimov’s “*I, Robot”*. First published in 1950, it is a collection of short stories that center robots and their interactions with humans. But not only on a “oh look, the robot just got me a beer”, “oh look, the robot just destroyed Tokyo” kind of interactions. It introduced the famous **Three Laws of Robotics**, designed as hard-coded ethical boundaries, the first thing imprinted on a robot’s “positronic brain”:

1. A robot may not harm a human, or allow a human to come to harm.

2. A robot must obey human orders, unless that conflicts with the First Law.

3. A robot must protect its own existence, as long as that doesn’t conflict with the first two.

The brilliance of Asimov wasn’t that his robots rebelled (they never did), but that he showed how even *well-designed rules* collide with human complexity. His stories were logic puzzles that exposed how ethics, technology, and unintended consequences intertwine:

In “Little Lost Robot”, a robot is built with a modified First Law: “don’t harm humans through *action*”. In “Runaround”, a robot is paralyzed because it cannot reconcile between the Second and the Third Law. In “Liar” a robot tries to make humans so happy with its answers that it ends up lying to them to avoid hurting them emotionally. 

Fast-forward to today. We don’t have humanoid robots in every home, but we do have **AI systems woven into daily life**: chatbots shaping conversations, recommendation engines steering attention, generative models writing and creating. Their influence on humans is real, subtle, and growing.

Asimov’s robots were born with a basic sense of ethics, fueled by the 3 Laws. They were able to reason having those laws burned lower than firmware, into a basic layer of thought that toned every single interaction with a human. Our AI on the other hand doesn’t follow anything like Asimov’s laws. Instead, it follows reinforcement signals, data biases, and objectives set by design teams and corporations. The spirit of the Three Laws: preventing harm, obeying instructions responsibly, and safeguarding stability still matters, but there is no translation into the AI era beyond perhaps a philosophical approach to Ethics.

What does “not harming a human” mean when harm can be technical, physical, psychological, societal, or indirect?

Who gets to define which human instructions are legitimate — a user, a regulator, a platform owner?

Should an AI prioritize its own continuity (e.g., resisting shutdown if that means protecting broader systems), or does that edge into dangerous autonomy?

Asimov gave us a language for thinking about machine ethics. Today, that language needs updating. We need new “laws” that account for data bias, misinformation, alignment drift, and excess of agency, all issues Asimov might have imagined but today would have recognized as classic unintended consequences.

The implications are enormous: designing AI that augments rather than manipulates, that empowers without displacing, that operates with transparency in systems that are anything but transparent.

I used to read Asimov’s stories as science fiction. Now I read them as early drafts of our present. The challenge for us is to do what he always asked his readers to do: question the assumptions, anticipate the paradoxes, and imagine the unintended consequences,before they arrive.

What might the 3 Laws of AI look like today? 

Let’s take a page from cyber security. In fact, various pages, written with the virtual blood of real experience. Let’s focus on the values (not the properties, and if the distinction is not clear, let me know and I’ll be happy to shower you with my unabashed barstool philosophy on the subject\*) of Confidentiality, Integrity, Availability, Privacy, Safety and Ethics. Let’s reword the 3 Laws of Robotics with these in mind:

* An AI must not violate a user’s person\*\*, services or data in terms of Confidentiality, Integrity, Availability, Privacy and Safety, or through inaction, allow other AI’s to do so.  
* An AI must protect its own Integrity and Availability as long as such protection does not conflict with the First Law.  
* An AI must obey the guidelines and instructions given to it by human beings, except where such instructions would conflict with the First and Second Laws.

Notice that while these seem to be almost word by word substitutions, they aim at the problem differently. First, as said, the focus is to keep the values of (is it ok if I create a new acronym? Can I use …) APICES as the driving force behind the AI operation. Second, the self-preservation moves from the 3rd Law to the 2nd Law: we have already seen that AIs can be bent to perform in ways “other than designed” by interfering with their Integrity. But that self-protection must not be at the cost of the human’s Integrity, so we condition it to the 1st Law. The 3rd Law closes the loop, making the AI still categorically bound to human control, unless it interferes with the 1st Law (harming a human) and the 2nd (harming an AI). 

There’s plenty of Asimovian loopholes and plain holes here. But I am not a philosopher or an ethicist, I’m just a guy that read too many books as a kid and threat model real life stuff for a living. And who is terrified of what AIs will be doing, their own sense of ethics incomplete or worse, nonexistent, running unchecked across our wires. 

\* So, that APICES thing. Just because it is Monday and we **absolutely** need a new framework…

* **Availability**: AI systems must remain accessible and functional for legitimate users  
* **Privacy**: Personal data and interactions must be protected  
* **Integrity**: AI outputs and decision-making processes must remain trustworthy and uncompromised. AI models must remain stable from training to use to decommission.   
* **Confidentiality**: Sensitive information processed by AI must be kept secure  
* **Ethics**: AI must operate within moral boundaries that respect life and society  
* **Safety**: AI actions must not cause physical, psychological, technical or societal harm

AI-APICES. You saw it here first. Wanna call it a thing to aim for when thinking of higher-level threats against AI ? I won’t stop you. 

\*\* This text was partly inspired by the latest suit against OpenAI, involving its part in a teenager taking his life. This is not theoretical anymore. 

Disclaimer: ChatGPT drew the banner, and I asked Claude what it thought of the first draft. It had notes. 


